{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68abd828",
   "metadata": {},
   "source": [
    "# 6강. 인공작사가 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dfd80d",
   "metadata": {},
   "source": [
    "## 6-3. I 다음 am을 쓰면 반 이상은 맞더라(통계)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7bd0be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sourse 문장: <start>나는 밥을 먹었다\n",
      "Target 문장: 나는 밥을 먹었다<end>\n"
     ]
    }
   ],
   "source": [
    "sentence = \"나는 밥을 먹었다\"\n",
    "\n",
    "source_sentence = \"<start>\" + sentence\n",
    "target_sentence = sentence + \"<end>\"\n",
    "# <start>는 문장의 시작\n",
    "# <start>토큰을 받은 순환 신경망은 \"나는\" 출력\n",
    "# 출력된 \"나는\"을 다시 입력, ... 반복을 통해 \"먹었다\"까지 출력\n",
    "# 마지막으로 완성을 뜻하는 <end> 토큰을 생성하여 마무리\n",
    "\n",
    "print(\"Sourse 문장:\", source_sentence)\n",
    "print(\"Target 문장:\", target_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585ba5b9",
   "metadata": {},
   "source": [
    "## 6-4. 연극대사 생성 인공지능 만들기_데이터 다듬기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69df65b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['First Citizen:', 'Before we proceed any further, hear me speak.', '', 'All:', 'Speak, speak.', '', 'First Citizen:', 'You are all resolved rather to die than to famish?', '']\n"
     ]
    }
   ],
   "source": [
    "import os, re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "file_path = os.getenv(\"HOME\") + '/aiffel/lyricist/data/shakespeare.txt'\n",
    "with open(file_path, \"r\") as f:\n",
    "    raw_corpus = f.read().splitlines()\n",
    "    \n",
    "print(raw_corpus[:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "630b80b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before we proceed any further, hear me speak.\n",
      "Speak, speak.\n",
      "You are all resolved rather to die than to famish?\n"
     ]
    }
   ],
   "source": [
    "# 필요없는 문장 지우기\n",
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0: continue # 길이가 0인 문장 건너뛰기\n",
    "    if sentence[-1] == ':': continue # 문장의 뒤에서 첫번째 문자가 :인 문장 건너뛰기\n",
    "        \n",
    "    if idx > 9 : break # 문장 10개만 확인\n",
    "        \n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1831f304",
   "metadata": {},
   "source": [
    "화자가 표기된 문장(0,3,6), 공백인 문장(2,5,9)은 필요가 없으므로 지워주기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da3a259f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence . <end>\n"
     ]
    }
   ],
   "source": [
    "# 문장 전처리\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() # 소문자로 바꾸고, 양쪽 공백 지우기\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 특수문자 양쪽에 공백 넣기\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 여러개의 공백은 하나의 공백으로 바꾸기\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 바꾸기\n",
    "    sentence = sentence.strip() # 다시 양쪽 공백 지우기\n",
    "    sentence = '<start> ' + sentence + ' <end>' # 문장 시작에는 <start>, 끝에는 <end>추가\n",
    "    return sentence\n",
    "    \n",
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882baebb",
   "metadata": {},
   "source": [
    "토큰화(Tokenize) : 문장을 일정한 기준으로 쪼개기  \n",
    "띄어쓰기를 기준으로 쪼개기\n",
    "1. 문장부호 -> 문장 부호 양쪽에 공백 추가\n",
    "2. 대소문자 -> 모든 문자들을 소문자로 변환\n",
    "3. 특수문자 -> 특수문자 모두 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f59e5802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> before we proceed any further , hear me speak . <end>',\n",
       " '<start> speak , speak . <end>',\n",
       " '<start> you are all resolved rather to die than to famish ? <end>',\n",
       " '<start> resolved . resolved . <end>',\n",
       " '<start> first , you know caius marcius is chief enemy to the people . <end>',\n",
       " '<start> we know t , we know t . <end>',\n",
       " '<start> let us kill him , and we ll have corn at our own price . <end>',\n",
       " '<start> is t a verdict ? <end>',\n",
       " '<start> no more talking on t let it be done away , away ! <end>',\n",
       " '<start> one word , good citizens . <end>']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정제 데이터 구축하기\n",
    "corpus = []\n",
    "\n",
    "# raw_corpus list에 저장된 문장들을 순서대로 반환하여 sentence에 저장\n",
    "for sentence in raw_corpus:\n",
    "    # 우리가 원하지 않는 문장은 건너뛰기\n",
    "    if len(sentence) == 0: continue\n",
    "    if sentence[-1] == \":\": continue\n",
    "    \n",
    "    # 앞서 구현한 preprocess_sentence() 함수를 이용하여 문장을 정제를 하고 담아주기\n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    corpus.append(preprocessed_sentence)\n",
    "        \n",
    "# 정제된 결과를 10개만 확인\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "898fb72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2  143   40 ...    0    0    0]\n",
      " [   2  110    4 ...    0    0    0]\n",
      " [   2   11   50 ...    0    0    0]\n",
      " ...\n",
      " [   2  149 4553 ...    0    0    0]\n",
      " [   2   34   71 ...    0    0    0]\n",
      " [   2  945   34 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f0cf096de50>\n"
     ]
    }
   ],
   "source": [
    "# 7000단어를 기억할 수 있는 tokenizer 만들기\n",
    "def tokenize(corpus):\n",
    "    # 이미 문장을 정제했기 때문에 filters는 필요없음\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=7000, \n",
    "        filters=' ',\n",
    "        oov_token=\"<unk>\" # 7000단어에 포함되지 못한 단어는 '<unk>'로 바꾸기\n",
    "    )\n",
    "    \n",
    "    # corpus를 이용해 tokenizer 내부의 단어장을 완성\n",
    "    # tokenizer.fit_on_texts(texts): 문자 데이터를 입력받아 리스트의 형태로 변환하는 메서드\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    \n",
    "    # 준비한 tokenizer를 이용해 corpus를 Tensor로 변환\n",
    "    # tokenizer.texts_to_sequences(texts): 텍스트 안의 단어들을 숫자의 시퀀스 형태로 변환하는 메서드\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)\n",
    "    \n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게 맞춰주기\n",
    "    # 만약 시퀀스가 짧다면 문장 뒤에 패딩을 붙여 길이를 맞춰주기(문장 앞에 패딩을 붙여 길이를 맞추고 싶다면 padding='pre'를 사용)\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  \n",
    "    \n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909accfa",
   "metadata": {},
   "source": [
    "- tf.keras.preprocessing.text.Tokenizer 패키지 : 정제된 데이터를 토큰화하고, 단어 사전을 만들어주며, 데이터를 숫자로 변환(벡터화 vectorize) / 숫자로 변환된 데이터 = 텐서(tensor)  \n",
    "- 텐서플로우의 Tokenizer와 pad_sequences를 사용 -> 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "94fbb26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2  143   40  933  140  591    4  124   24  110]\n",
      " [   2  110    4  110    5    3    0    0    0    0]\n",
      " [   2   11   50   43 1201  316    9  201   74    9]]\n"
     ]
    }
   ],
   "source": [
    "# 3번째 행, 10번째 열까지만 출력\n",
    "print(tensor[:3, :10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1f73da",
   "metadata": {},
   "source": [
    "텐서 데이터는 모두 정수로 이루어져 있다. 이 숫자는 tokenizer에 구축된 단어 사전의 인덱스이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "12969099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : ,\n",
      "5 : .\n",
      "6 : the\n",
      "7 : and\n",
      "8 : i\n",
      "9 : to\n",
      "10 : of\n"
     ]
    }
   ],
   "source": [
    "# tokenizer.index_word: 현재 계산된 단어의 인덱스와 인덱스에 해당하는 단어를 dictionary 형대로 반환 (Ex. {index: '~~', index: '~~', ...})\n",
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a0511c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2 143  40 933 140 591   4 124  24 110   5   3   0   0   0   0   0   0\n",
      "   0   0]\n",
      "[143  40 933 140 591   4 124  24 110   5   3   0   0   0   0   0   0   0\n",
      "   0   0]\n"
     ]
    }
   ],
   "source": [
    "# tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성합니다\n",
    "# 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높습니다.\n",
    "src_input = tensor[:, :-1]  \n",
    "# tensor에서 <start>를 잘라내서 타겟 문장을 생성합니다.\n",
    "tgt_input = tensor[:, 1:]    \n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce58d28",
   "metadata": {},
   "source": [
    "텐서 출력부에서 행 뒤쪽에 0이 많이 나온 부분은 정해진 입력 시퀀스 길이보다 문장이 짧을 경우 0으로 패딩(padding)을 채워 넣은 것. 사전에는 없지만 0은 바로 패딩 문자 <pad>가 될 것이다.  \n",
    "소스는 2(start)에서 시작해서 3(end)으로 끝난 후 0(pad)로 채워져 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "816502b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 20), (256, 20)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터셋 객체를 생성\n",
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    " # tokenizer가 구축한 단어사전 내 7000개와, 여기 포함되지 않은 0:<pad>를 포함하여 7001개\n",
    " # tokenizer.num_words: 주어진 데이터의 문장들에서 빈도수가 높은 n개의 단어만 선택\n",
    " # tokenize() 함수에서 num_words를 7000개로 선언했기 때문에, tokenizer.num_words의 값은 7000\n",
    "VOCAB_SIZE = tokenizer.num_words + 1   \n",
    "\n",
    "# 준비한 데이터 소스로부터 데이터셋을 만듭니다\n",
    "# 데이터셋에 대해서는 아래 문서를 참고하세요\n",
    "# 자세히 알아둘수록 도움이 많이 되는 중요한 문서입니다\n",
    "# https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c4cc30",
   "metadata": {},
   "source": [
    "텐서플로우는 model.fit()형태로 numpy array 데이터셋을 생성해 model에 제공하는 형태의 학습이 아닌 텐서로 생성된 데이터를 이용해 tf.data.Dataset객체를 생성하는 방법 사용.  \n",
    "tf.data.Dataset.from_tensor_slices() 메소드를 이용해 tf.data.Dataset객체를 생성."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d102904",
   "metadata": {},
   "source": [
    "#### 데이터셋 생성 과정\n",
    "- 정규표현식을 이용한 corpus 생성\n",
    "- tf.keras.preprocessing.text.Tokenizer를 이용해 corpus를 텐서로 변환\n",
    "- tf.data.Dataset.from_tensor_slices()를 이용해 corpus 텐서를 tf.data.Dataset객체로 변환\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8f5ec3",
   "metadata": {},
   "source": [
    "## 6-5. 연극대사 생성 인공지능 만들기_인공지능 학습시키기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9f87bb",
   "metadata": {},
   "source": [
    "![RNN](https://user-images.githubusercontent.com/116326867/203905081-1ea71d25-4dcf-45b3-b8b5-38eefa209bf4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96df30b4",
   "metadata": {},
   "source": [
    "1개의 Embedding 레이어, 2개의 LSTM 레이어, 1개의 Dense 레이어로 구성  \n",
    "tf.keras.Model을 Subclassing하는 방식으로 모델 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2df86ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        # 1개의 Embedding 레이어, 2개의 LSTM 레이어, 1개의 Dense 레이어로 구성\n",
    "        # Embedding 레이어는 단어 사전의 인덱스 값을 해당 인덱스 번째의 워드 벡터로 바꿔준다.\n",
    "        # 이 워드 벡터는 의미 벡터 공간에서 단어의 추상적 표현으로 사용된다. \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size) \n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)  \n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "# embedding size 값이 커질수록 단어의 추상적인 특징들을 더 잡아낼 수 있지만\n",
    "# 그만큼 충분한 데이터가 없으면 안좋은 결과 값을 가져온다.  \n",
    "embedding_size = 256 # 워드 벡터의 차원수, 단어가 추상적으로 표현되는 크기\n",
    "hidden_size = 1024 # 모델에 얼마나 많은 일꾼을 둘 것인가? 정도로 이해하면 좋다.\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size) # tokenizer.num_words에 +1인 이유는 문장에 없는 pad가 사용되었기 때문이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4611b4",
   "metadata": {},
   "source": [
    "입력 텐서에 들어 있는 단어 사전의 인덱스 -> Embedding 레이어 : 인덱스 값을 해당 인덱스 번째의 워드 벡터로 바꿔주기 -> 워드 벡터는 의미 벡터 공간에서 단어의 추상적 표현(representation)으로 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "02f8ebea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 20, 7001), dtype=float32, numpy=\n",
       "array([[[-1.60995420e-04,  6.51997179e-05, -2.83668574e-04, ...,\n",
       "         -3.76122625e-05, -1.59228672e-04,  6.81756137e-05],\n",
       "        [-7.26288126e-05, -1.42863528e-05, -7.00400269e-04, ...,\n",
       "          2.10009603e-04, -1.41425917e-04,  6.25086523e-06],\n",
       "        [-1.87466765e-04, -1.18938944e-04, -1.25731435e-03, ...,\n",
       "          2.80524750e-04, -3.65418731e-04,  4.08592314e-04],\n",
       "        ...,\n",
       "        [-1.60256808e-03,  1.28849142e-03,  3.94108193e-03, ...,\n",
       "         -9.01996042e-04,  7.79986382e-04, -1.44647865e-03],\n",
       "        [-1.98059413e-03,  1.52819534e-03,  4.73632896e-03, ...,\n",
       "         -1.17884774e-03,  9.58196237e-04, -1.56709692e-03],\n",
       "        [-2.36140564e-03,  1.71841262e-03,  5.49159711e-03, ...,\n",
       "         -1.47687609e-03,  1.13567058e-03, -1.66125677e-03]],\n",
       "\n",
       "       [[-1.60995420e-04,  6.51997179e-05, -2.83668574e-04, ...,\n",
       "         -3.76122625e-05, -1.59228672e-04,  6.81756137e-05],\n",
       "        [-1.85427809e-04, -2.79486907e-04, -1.76117872e-04, ...,\n",
       "          6.49645299e-05, -1.32199028e-04,  4.98747613e-05],\n",
       "        [-3.02310538e-04, -6.64111227e-04, -3.67189205e-04, ...,\n",
       "          2.79924338e-04, -2.07198769e-04, -4.75658744e-05],\n",
       "        ...,\n",
       "        [-1.14820455e-03,  1.58235116e-03,  3.26587609e-03, ...,\n",
       "         -1.12554943e-03,  6.26420369e-04, -2.06320826e-03],\n",
       "        [-1.51634624e-03,  1.79909973e-03,  4.03072452e-03, ...,\n",
       "         -1.30260619e-03,  8.32298538e-04, -2.08609737e-03],\n",
       "        [-1.90075894e-03,  1.97544345e-03,  4.78807418e-03, ...,\n",
       "         -1.51889212e-03,  1.02590176e-03, -2.09308625e-03]],\n",
       "\n",
       "       [[-1.60995420e-04,  6.51997179e-05, -2.83668574e-04, ...,\n",
       "         -3.76122625e-05, -1.59228672e-04,  6.81756137e-05],\n",
       "        [-8.66923656e-05,  1.24590137e-04, -4.72600077e-04, ...,\n",
       "          2.89050458e-05, -2.66882445e-04,  2.06961529e-04],\n",
       "        [-1.39159441e-04,  3.86158586e-04, -5.10466460e-04, ...,\n",
       "          1.37146504e-04, -2.85502319e-04,  2.80370750e-05],\n",
       "        ...,\n",
       "        [-3.53077310e-03,  1.84447423e-03,  7.37481564e-03, ...,\n",
       "         -2.47265678e-03,  1.75289146e-03, -2.33282358e-03],\n",
       "        [-3.79084633e-03,  1.94040732e-03,  7.89001677e-03, ...,\n",
       "         -2.75297416e-03,  1.89050578e-03, -2.28821789e-03],\n",
       "        [-4.02133539e-03,  2.02014623e-03,  8.33969004e-03, ...,\n",
       "         -3.01679550e-03,  2.01678486e-03, -2.24758871e-03]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-1.60995420e-04,  6.51997179e-05, -2.83668574e-04, ...,\n",
       "         -3.76122625e-05, -1.59228672e-04,  6.81756137e-05],\n",
       "        [-3.19476123e-04,  3.19607178e-04, -6.38028723e-04, ...,\n",
       "          7.23477660e-06, -2.91120698e-04,  1.19670440e-04],\n",
       "        [-2.94396887e-04,  2.63836671e-04, -6.84361847e-04, ...,\n",
       "          2.45886738e-04, -5.41083224e-04, -2.23915820e-04],\n",
       "        ...,\n",
       "        [-1.98777765e-03,  2.01052683e-03,  5.65587031e-03, ...,\n",
       "         -1.53767609e-03,  1.81548740e-03, -1.77675893e-03],\n",
       "        [-2.35757558e-03,  2.09553959e-03,  6.31609978e-03, ...,\n",
       "         -1.82904128e-03,  1.92331092e-03, -1.82439794e-03],\n",
       "        [-2.71068607e-03,  2.16285838e-03,  6.92079961e-03, ...,\n",
       "         -2.12625694e-03,  2.02685525e-03, -1.86248310e-03]],\n",
       "\n",
       "       [[-1.60995420e-04,  6.51997179e-05, -2.83668574e-04, ...,\n",
       "         -3.76122625e-05, -1.59228672e-04,  6.81756137e-05],\n",
       "        [-3.19476123e-04,  3.19607178e-04, -6.38028723e-04, ...,\n",
       "          7.23477660e-06, -2.91120698e-04,  1.19670440e-04],\n",
       "        [-4.15920251e-04,  4.24968865e-04, -7.42680219e-04, ...,\n",
       "         -1.41003839e-04, -3.21864442e-04,  1.49824948e-04],\n",
       "        ...,\n",
       "        [-8.73624580e-04,  2.18756520e-03,  2.64961575e-03, ...,\n",
       "         -1.95694811e-05,  8.23117094e-04, -1.84886868e-03],\n",
       "        [-1.23470859e-03,  2.27452768e-03,  3.43352859e-03, ...,\n",
       "         -2.92011275e-04,  1.00439787e-03, -1.92925427e-03],\n",
       "        [-1.61724573e-03,  2.34045950e-03,  4.21404419e-03, ...,\n",
       "         -6.03550056e-04,  1.17991574e-03, -1.98377506e-03]],\n",
       "\n",
       "       [[-1.60995420e-04,  6.51997179e-05, -2.83668574e-04, ...,\n",
       "         -3.76122625e-05, -1.59228672e-04,  6.81756137e-05],\n",
       "        [-6.45615219e-04, -1.69282823e-04, -4.24881466e-04, ...,\n",
       "         -2.08344936e-04, -2.65213428e-04, -1.21988400e-04],\n",
       "        [-8.30015284e-04, -1.80571558e-04, -9.88822940e-05, ...,\n",
       "         -2.00029317e-04, -2.29536672e-04, -4.04894032e-04],\n",
       "        ...,\n",
       "        [-2.33549532e-03,  1.17299601e-03,  4.94557107e-03, ...,\n",
       "         -1.67294708e-03,  1.35589472e-03, -1.97996409e-03],\n",
       "        [-2.66896631e-03,  1.37994241e-03,  5.66576375e-03, ...,\n",
       "         -1.88350049e-03,  1.46151951e-03, -2.00534891e-03],\n",
       "        [-2.98847910e-03,  1.55311357e-03,  6.33399794e-03, ...,\n",
       "         -2.12223269e-03,  1.57667208e-03, -2.02071876e-03]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터셋에서 데이터 한 배치만 불러오는 방법(model에 데이터를 아주 조금 넣어보는 것)\n",
    "# model의 input shape이 결정되면서 model.build()가 자동으로 호출됨.\n",
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "\n",
    "# 한 배치만 불러온 데이터를 모델에 넣어보기\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d85941b",
   "metadata": {},
   "source": [
    "shape = (256, 20, 7001)  \n",
    "- 7001 : Dense 레이어의 출력 차원수(어느 단어의 확률이 가장 높을지 모델링)  \n",
    "- 256 : 이전 스텝에서 지정한 배치 사이즈  \n",
    "- 20 : LSTM에서 자신에게 입력된 시퀀스의 길이(데이터를 입력받으면서 알게됨)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "91539614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  1792256   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  5246976   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  7176025   \n",
      "=================================================================\n",
      "Total params: 22,607,961\n",
      "Trainable params: 22,607,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모델의 구조를 확인\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b120d3",
   "metadata": {},
   "source": [
    "시퀀스의 길이를 모르기 때문에 Output Shape 특정 불가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c004222a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "93/93 [==============================] - 20s 194ms/step - loss: 3.4655\n",
      "Epoch 2/30\n",
      "93/93 [==============================] - 18s 192ms/step - loss: 2.7990\n",
      "Epoch 3/30\n",
      "93/93 [==============================] - 17s 184ms/step - loss: 2.6786\n",
      "Epoch 4/30\n",
      "93/93 [==============================] - 17s 182ms/step - loss: 2.5754\n",
      "Epoch 5/30\n",
      "93/93 [==============================] - 17s 186ms/step - loss: 2.5166\n",
      "Epoch 6/30\n",
      "93/93 [==============================] - 18s 190ms/step - loss: 2.4599\n",
      "Epoch 7/30\n",
      "93/93 [==============================] - 17s 187ms/step - loss: 2.4031\n",
      "Epoch 8/30\n",
      "93/93 [==============================] - 17s 185ms/step - loss: 2.3493\n",
      "Epoch 9/30\n",
      "93/93 [==============================] - 17s 186ms/step - loss: 2.2965\n",
      "Epoch 10/30\n",
      "93/93 [==============================] - 17s 188ms/step - loss: 2.2443\n",
      "Epoch 11/30\n",
      "93/93 [==============================] - 17s 188ms/step - loss: 2.1949\n",
      "Epoch 12/30\n",
      "93/93 [==============================] - 17s 187ms/step - loss: 2.1453\n",
      "Epoch 13/30\n",
      "93/93 [==============================] - 17s 186ms/step - loss: 2.0954\n",
      "Epoch 14/30\n",
      "93/93 [==============================] - 17s 186ms/step - loss: 2.0451\n",
      "Epoch 15/30\n",
      "93/93 [==============================] - 17s 187ms/step - loss: 1.9949\n",
      "Epoch 16/30\n",
      "93/93 [==============================] - 17s 187ms/step - loss: 1.9454\n",
      "Epoch 17/30\n",
      "93/93 [==============================] - 17s 188ms/step - loss: 1.8947\n",
      "Epoch 18/30\n",
      "93/93 [==============================] - 17s 188ms/step - loss: 1.8438\n",
      "Epoch 19/30\n",
      "93/93 [==============================] - 17s 187ms/step - loss: 1.7913\n",
      "Epoch 20/30\n",
      "93/93 [==============================] - 17s 186ms/step - loss: 1.7385\n",
      "Epoch 21/30\n",
      "93/93 [==============================] - 17s 186ms/step - loss: 1.6866\n",
      "Epoch 22/30\n",
      "93/93 [==============================] - 17s 187ms/step - loss: 1.6337\n",
      "Epoch 23/30\n",
      "93/93 [==============================] - 17s 187ms/step - loss: 1.5819\n",
      "Epoch 24/30\n",
      "93/93 [==============================] - 17s 188ms/step - loss: 1.5286\n",
      "Epoch 25/30\n",
      "93/93 [==============================] - 18s 188ms/step - loss: 1.4752\n",
      "Epoch 26/30\n",
      "93/93 [==============================] - 17s 188ms/step - loss: 1.4220\n",
      "Epoch 27/30\n",
      "93/93 [==============================] - 17s 188ms/step - loss: 1.3678\n",
      "Epoch 28/30\n",
      "93/93 [==============================] - 17s 187ms/step - loss: 1.3124\n",
      "Epoch 29/30\n",
      "93/93 [==============================] - 17s 187ms/step - loss: 1.2584\n",
      "Epoch 30/30\n",
      "93/93 [==============================] - 17s 187ms/step - loss: 1.2029\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0cf09d9190>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam() \n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy( # 훈련 데이터의 라벨이 정수의 형태로 제공될 때 사용하는 손실함수\n",
    "    from_logits=True, # 기본값은 False. 모델에 의해 생성된 출력 값이 정규화되지 않았음을 손실 함수에 알려준다.(softmax함수가 적용되지 않았다는걸 의미) \n",
    "    reduction='none'  # 기본값은 SUM. 각자 나오는 값의 반환 원할 때 None을 사용한다.\n",
    ")\n",
    "# 모델을 학습시키키 위한 학습과정을 설정하는 단계\n",
    "model.compile(loss=loss, optimizer=optimizer) # 손실함수와 훈련과정을 설정\n",
    "model.fit(dataset, epochs=30) # 만들어둔 데이터셋으로 모델을 학습.(30번 학습을 반복)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a436931",
   "metadata": {},
   "source": [
    "참고 : [optimizer, loss1](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers) / [optimizer, loss2](https://www.tensorflow.org/api_docs/python/tf/keras/losses)  \n",
    "어떤 optimizier를 써야할지 모른다면 현재 가장 많이 쓰는 Adam을 써보는 것도 방법.  \n",
    "얼마나 틀리는지(loss)를 알게하는 함수가 손실함수, 손실함수의 최소값을 찾아가는 과정을 optimization, 이를 수행하는 알고리즘을 optimizer(최적화)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167c8960",
   "metadata": {},
   "source": [
    "## 6-6. 연극대사 생성 인공지능 만들기_평가하기\n",
    "평가방법 : BLEU, ROUGE, 사람이 평가하기 등  \n",
    "자연어 처리에서 결과를 평가할 때는 어떤 단어가 포함 되었는지를 바탕으로 측정."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dd0b5ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델에게 시작 문장을 전달하면 모델이 시작 문장을 바탕으로 작문을 진행하는 함수만들기\n",
    "def generate_text(model, tokenizer, init_sentence='<start>', max_len=20): # 시작 문자열 디폴트는 <start>\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence]) # init_sentence 텍스트 안의 단어들을 숫자의 시퀀스의 형태로 변환\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64) # 텐서로 변환\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 단어 하나씩 예측해 문장 만들기(루프를 돌면서 init_sentence에 단어를 하나씩 생성)\n",
    "    while True:\n",
    "        predict = model(test_tensor) # 입력받은 문장의 텐서 입력\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] # 예측된 값 중 가장 높은 확률인 word index 뽑아내기\n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1) # 예측된 word index를 문장 뒤에 붙이기\n",
    "        if predict_word.numpy()[0] == end_token: break # 모델이 <end>를 예측했거나\n",
    "        if test_tensor.shape[1] >= max_len: break # max_len에 도달했다면 문장 생성을 마침\n",
    "            \n",
    "    generated = \"\"\n",
    "    # tokenizer를 이용해 word index를 단어로 하나씩 변환\n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated #최종적으로 모델이 생성한 문장을 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646d1203",
   "metadata": {},
   "source": [
    "학습 단계에서는 while문이 필요 없다. 소스 문장과 타겟 문장이 있고 소스 문장을 모델에 입력해서 나온 결과를 타겟 문장과 직접 비교하면 되기 때문.  \n",
    "BUT 테스트 단계에서는 while문 필요. 텍스트 생성 택스크를 위한 테스트 데이터셋을 따로 생성한 적이 없어 소스 문장과 타겟 문장 없기 때문."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5c161ade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> he is not lolling on a lewd day bed , <end> '"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 시작문장으로 he를 넣어 문장생성 함수 실행\n",
    "generate_text(model, tokenizer, init_sentence=\"<start> he\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebacc5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b63fd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83326e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d24797d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09397120",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
