{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58cb0f4a",
   "metadata": {},
   "source": [
    "# 인공 작사가 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aec5bce",
   "metadata": {},
   "source": [
    "### 라이브러리 버전 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9a394ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "import glob # glob 함수 : 사용자가 제시한 조건에 맞는 파일명을 리스트 형식으로 반환\n",
    "import tensorflow\n",
    "\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6c3f70",
   "metadata": {},
   "source": [
    "## Step 1. 데이터 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "969081ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ln: failed to create symbolic link '/aiffel/aiffel/lyricist/data/data': Read-only file system\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p ~/aiffel/lyricist/models\n",
    "!ln -s ~/data ~/aiffel/lyricist/data\n",
    "# 이미 이전에 불러와서 failed가 떴다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295e273c",
   "metadata": {},
   "source": [
    "## Step 2. 데이터 읽어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9d753a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " [\"Busted flat in Baton Rouge, waitin' for a train\", \"And I's feelin' near as faded as my jeans\", 'Bobby thumbed a diesel down, just before it rained']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os, re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path) # txt_file_path 경로에 있는 모든 파일명을 리스트 형식으로 txt_list에 할당\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines() # splitlines()  : 여러라인으로 구분되어 있는 문자열을 한라인씩 분리하여 리스트로 반환\n",
    "        raw_corpus.extend(raw) \n",
    "        \n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2769278",
   "metadata": {},
   "source": [
    "## Step 3. 데이터 정제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "090f6205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Busted flat in Baton Rouge, waitin' for a train\n",
      "And I's feelin' near as faded as my jeans\n",
      "Bobby thumbed a diesel down, just before it rained\n",
      "It rode us all the way to New Orleans I pulled my harpoon out of my dirty red bandanna\n",
      "I was playin' soft while Bobby sang the blues, yeah\n",
      "Windshield wipers slappin' time, I was holdin' Bobby's hand in mine\n",
      "We sang every song that driver knew Freedom's just another word for nothin' left to lose\n",
      "Nothin', don't mean nothin' hon' if it ain't free, no no\n",
      "And, feelin' good was easy, Lord, when he sang the blues\n",
      "You know, feelin' good was good enough for me\n"
     ]
    }
   ],
   "source": [
    "# 필요없는 문장 지우기\n",
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0: continue # 길이가 0인 문장 건너뛰기\n",
    "        \n",
    "    if idx > 9 : break # 문장 10개만 확인\n",
    "        \n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6920b971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 전처리\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() # 소문자로 바꾸고, 양쪽 공백 지우기\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 특수문자 양쪽에 공백 넣기\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 여러개의 공백은 하나의 공백으로 바꾸기\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 바꾸기\n",
    "    sentence = sentence.strip() # 다시 양쪽 공백 지우기\n",
    "    sentence = '<start> ' + sentence + ' <end>' # 문장 시작에는 <start> , 끝에는 <end>추가\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b5628d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_split = preprocess_sentence(sentence).split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a29b57",
   "metadata": {},
   "source": [
    "- 공백 기준으로 분할하여 각 문장을 토큰화했을 때처럼 만들어주었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93d9cc26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<start> busted flat in baton rouge , waitin for a train <end>', '<start> and i s feelin near as faded as my jeans <end>', '<start> bobby thumbed a diesel down , just before it rained <end>', '<start> it rode us all the way to new orleans i pulled my harpoon out of my dirty red bandanna <end>', '<start> i was playin soft while bobby sang the blues , yeah <end>', '<start> windshield wipers slappin time , i was holdin bobby s hand in mine <end>', '<start> we sang every song that driver knew freedom s just another word for nothin left to lose <end>', '<start> nothin , don t mean nothin hon if it ain t free , no no <end>', '<start> and , feelin good was easy , lord , when he sang the blues <end>', '<start> you know , feelin good was good enough for me <end>']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "175986"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정제 데이터 구축하기\n",
    "corpus = []\n",
    "\n",
    "# raw_corpus list에 저장된 문장들을 순서대로 반환하여 sentence에 저장\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0: continue # 길이가 0인 문장 제외\n",
    "    if len(sentence_split) >= 17: continue # <start>, <end> 제외하고 토큰이 15개를 넘어가는 문장 제외\n",
    "    \n",
    "    # 앞서 구현한 preprocess_sentence() 함수를 이용하여 문장을 정제를 하고 담아주기\n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    corpus.append(preprocessed_sentence)\n",
    "\n",
    "# 정제된 결과를 10개만 확인\n",
    "print(corpus[:10])\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de9cfb1",
   "metadata": {},
   "source": [
    "## Step 4. 평가 데이터셋 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88b18474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2 3609 1692 ...    0    0    0]\n",
      " [   2    8    5 ...    0    0    0]\n",
      " [   2  804 7664 ...    0    0    0]\n",
      " ...\n",
      " [   2    5   22 ...    0    0    0]\n",
      " [   2    5   22 ...    0    0    0]\n",
      " [   2    5   22 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f8a4fa25040>\n"
     ]
    }
   ],
   "source": [
    "# 15,000단어를 기억할 수 있는 tokenizer 만들기\n",
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=15000,\n",
    "        filters=' ', # 이미 문장을 정제했기 떄문에 filters는 필요없음\n",
    "        oov_token=\"\" # <UNK> # 15000단어에 포함되지 못한 단어는 \"\"로 바꾸기\n",
    "    )\n",
    "    \n",
    "    # corpus를 이용해 tokenizer 내부의 단어장 완성\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    \n",
    "    # 준비한 tokenizer를 이용해 corpus를 Tensor로 변환\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)\n",
    "    \n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게 맞춰주기\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "    \n",
    "    print(tensor, tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e6da21",
   "metadata": {},
   "source": [
    "- 텐서플로우의 Tokenizer와 pad_sequences를 사용해서 토큰화할 수 있다.\n",
    "- tf.keras.preprocessing.text.Tokenizer 패키지 : 정제된 데이터를 토큰화하고, 단어 사전을 만들어주며, 데이터를 숫자로 변환(벡터화 vectorize)  \n",
    "- tokenizer.fit_on_texts(texts): 문자 데이터를 입력받아 리스트의 형태로 변환하는 메서드  \n",
    "- tokenizer.texts_to_sequences(texts): 텍스트 안의 단어들을 숫자의 시퀀스 형태로 변환하는 메서드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15a6a4b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2 3609 1692   14 7058 3995    4 1100   28    9  681    3    0    0]\n",
      " [   2    8    5   16  513  848   81 2589   81   13  948    3    0    0]\n",
      " [   2  804 7664    9 6048   60    4   36  185   11 4575    3    0    0]]\n"
     ]
    }
   ],
   "source": [
    "# 3번째 행, 14번째 열까지만 출력\n",
    "print(tensor[:3, :14])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9b02a3",
   "metadata": {},
   "source": [
    "- 텐서 데이터는 모두 정수로 이루어져 있으며, 이 숫자는 tokenizer에 구축된 단어 사전의 인덱스이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60041e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : \n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : ,\n",
      "5 : i\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n"
     ]
    }
   ],
   "source": [
    "# tokenizer.index_word: 현재 계산된 단어의 인덱스와 인덱스에 해당하는 단어를 dictionary 형태로 반환\n",
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7d563ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2 3609 1692   14 7058 3995    4 1100   28    9  681    3    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0]\n",
      "[3609 1692   14 7058 3995    4 1100   28    9  681    3    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "# tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성하기\n",
    "# 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높다.\n",
    "src_input = tensor[:, :-1]  \n",
    "# tensor에서 <start>를 잘라내서 타겟 문장을 생성합니다.\n",
    "tgt_input = tensor[:, 1:]    \n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3c6f68c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(140788, 346) (140788, 346)\n",
      "(35198, 346) (35198, 346)\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size=0.2, random_state=20)\n",
    "\n",
    "print(enc_train.shape, dec_train.shape)\n",
    "print(enc_val.shape, dec_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f62b9f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 346), (256, 346)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터셋 객체를 생성\n",
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    " # tokenizer가 구축한 단어사전 내 7000개와, 여기 포함되지 않은 0:를 포함하여 7001개\n",
    " # tokenizer.num_words: 주어진 데이터의 문장들에서 빈도수가 높은 n개의 단어만 선택\n",
    " # tokenize() 함수에서 num_words를 15000개로 선언했기 때문에, tokenizer.num_words의 값은 7000\n",
    "VOCAB_SIZE = tokenizer.num_words + 1   \n",
    "\n",
    "# 준비한 데이터 소스로부터 데이터셋을 만듭니다\n",
    "# 데이터셋에 대해서는 아래 문서를 참고하세요\n",
    "# 자세히 알아둘수록 도움이 많이 되는 중요한 문서입니다\n",
    "# https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff8ff61",
   "metadata": {},
   "source": [
    "## Step 5. 인공지능 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "885a84a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        # 1개의 Embedding 레이어, 2개의 LSTM 레이어, 1개의 Dense 레이어로 구성\n",
    "        # Embedding 레이어는 단어 사전의 인덱스 값을 해당 인덱스 번째의 워드 벡터로 바꿔준다.\n",
    "        # 이 워드 벡터는 의미 벡터 공간에서 단어의 추상적 표현으로 사용된다. \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size) \n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)  \n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "# embedding size 값이 커질수록 단어의 추상적인 특징들을 더 잡아낼 수 있지만\n",
    "# 그만큼 충분한 데이터가 없으면 안좋은 결과 값을 가져온다.  \n",
    "embedding_size = 128 # 워드 벡터의 차원수, 단어가 추상적으로 표현되는 크기\n",
    "hidden_size = 128 # 모델에 얼마나 많은 일꾼을 둘 것인가? 정도로 이해하면 좋다.\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size) # tokenizer.num_words에 +1인 이유는 문장에 없는 pad가 사용되었기 때문이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c46ac9",
   "metadata": {},
   "source": [
    "- 입력 텐서에 들어 있는 단어 사전의 인덱스 -> Embedding 레이어 : 인덱스 값을 해당 인덱스 번째의 워드 벡터로 바꿔주기 -> 워드 벡터는 의미 벡터 공간에서 단어의 추상적 표현(representation)으로 사용  \n",
    "- embedding_size = 2048, hidden_size = 2048로 설정했을 때, 한 배치만 불러온 데이터를 모델에 넣어봤을 때, 크기가 너무 큰 나머지 resourceexhaustederror가 발생했다.  \n",
    "그래서 embedding_size = 1024, hidden_size = 1024로 설정했을 때, 한 배치만 불러온 데이터를 모델에 넣어봤을 때, 다시 resourceexhaustederror가 발생했다. 여전히 크기가 너무 컸던 거 같다.   \n",
    "embedding_size = 256, hidden_size = 1024로 설정했을 때에도 계속해서 같은 에러가 발생했고, 처음부터 다시 셀을 돌려보기로 했다. 그러면서 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "53549d2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 346, 15001), dtype=float32, numpy=\n",
       "array([[[ 1.20692406e-04,  6.11458381e-05,  5.36778170e-05, ...,\n",
       "         -2.62293561e-05, -2.12236555e-05,  4.41464108e-05],\n",
       "        [ 2.42754060e-04, -6.46849958e-06, -5.72653989e-05, ...,\n",
       "          3.90439855e-05, -6.51099690e-05, -6.69616929e-05],\n",
       "        [ 2.50645564e-04, -1.13424539e-04,  5.03541014e-06, ...,\n",
       "         -4.30872342e-06, -1.41379460e-05, -2.62799120e-04],\n",
       "        ...,\n",
       "        [-4.95947374e-04,  2.86488576e-05, -2.33744713e-03, ...,\n",
       "         -1.77308882e-03, -2.15978059e-03,  2.06259172e-03],\n",
       "        [-4.95947374e-04,  2.86488576e-05, -2.33744713e-03, ...,\n",
       "         -1.77308882e-03, -2.15978059e-03,  2.06259172e-03],\n",
       "        [-4.95947374e-04,  2.86488576e-05, -2.33744713e-03, ...,\n",
       "         -1.77308882e-03, -2.15978059e-03,  2.06259172e-03]],\n",
       "\n",
       "       [[ 1.20692406e-04,  6.11458381e-05,  5.36778170e-05, ...,\n",
       "         -2.62293561e-05, -2.12236555e-05,  4.41464108e-05],\n",
       "        [ 8.09638877e-05,  2.15759152e-04,  2.12190193e-04, ...,\n",
       "         -1.96579043e-04,  6.18806080e-05, -3.60787203e-06],\n",
       "        [-9.01953536e-05,  4.72116808e-04,  3.38198646e-04, ...,\n",
       "         -2.41074391e-04,  6.35308170e-05, -8.06454409e-05],\n",
       "        ...,\n",
       "        [-4.95946559e-04,  2.86489103e-05, -2.33744737e-03, ...,\n",
       "         -1.77308964e-03, -2.15978036e-03,  2.06259242e-03],\n",
       "        [-4.95946559e-04,  2.86489103e-05, -2.33744737e-03, ...,\n",
       "         -1.77308964e-03, -2.15978036e-03,  2.06259242e-03],\n",
       "        [-4.95946559e-04,  2.86489103e-05, -2.33744737e-03, ...,\n",
       "         -1.77308964e-03, -2.15978036e-03,  2.06259242e-03]],\n",
       "\n",
       "       [[ 1.20692406e-04,  6.11458381e-05,  5.36778170e-05, ...,\n",
       "         -2.62293561e-05, -2.12236555e-05,  4.41464108e-05],\n",
       "        [ 1.71158448e-04,  8.67134295e-05, -1.51917266e-04, ...,\n",
       "          4.20419892e-05, -1.09077155e-04,  1.23711361e-04],\n",
       "        [ 2.68432457e-04, -3.21681932e-06, -2.70042423e-04, ...,\n",
       "          1.15492483e-04, -2.13927808e-04,  9.98091127e-05],\n",
       "        ...,\n",
       "        [-4.95947548e-04,  2.86492068e-05, -2.33744760e-03, ...,\n",
       "         -1.77308812e-03, -2.15978082e-03,  2.06259149e-03],\n",
       "        [-4.95947548e-04,  2.86492068e-05, -2.33744760e-03, ...,\n",
       "         -1.77308812e-03, -2.15978082e-03,  2.06259149e-03],\n",
       "        [-4.95947548e-04,  2.86492068e-05, -2.33744760e-03, ...,\n",
       "         -1.77308812e-03, -2.15978082e-03,  2.06259149e-03]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 1.20692406e-04,  6.11458381e-05,  5.36778170e-05, ...,\n",
       "         -2.62293561e-05, -2.12236555e-05,  4.41464108e-05],\n",
       "        [ 5.12910701e-05,  1.31350898e-04,  1.79569703e-04, ...,\n",
       "         -9.46593573e-05,  8.19991510e-06,  7.96672757e-05],\n",
       "        [-2.90966545e-05,  2.27574157e-04,  3.10983072e-04, ...,\n",
       "         -1.41898301e-04, -9.57374868e-05,  1.80173229e-05],\n",
       "        ...,\n",
       "        [-4.95946733e-04,  2.86485174e-05, -2.33744760e-03, ...,\n",
       "         -1.77308952e-03, -2.15978059e-03,  2.06259172e-03],\n",
       "        [-4.95946733e-04,  2.86485174e-05, -2.33744760e-03, ...,\n",
       "         -1.77308952e-03, -2.15978059e-03,  2.06259172e-03],\n",
       "        [-4.95946733e-04,  2.86485174e-05, -2.33744760e-03, ...,\n",
       "         -1.77308952e-03, -2.15978059e-03,  2.06259172e-03]],\n",
       "\n",
       "       [[ 1.20692406e-04,  6.11458381e-05,  5.36778170e-05, ...,\n",
       "         -2.62293561e-05, -2.12236555e-05,  4.41464108e-05],\n",
       "        [ 1.95594010e-04,  8.50686120e-05,  2.14933913e-04, ...,\n",
       "          3.85298517e-06, -5.23428462e-05, -2.22885628e-05],\n",
       "        [ 4.58041846e-04,  1.48826890e-04,  4.83023701e-04, ...,\n",
       "         -1.03100501e-04, -5.41658301e-05, -6.10597781e-05],\n",
       "        ...,\n",
       "        [-4.95946617e-04,  2.86490849e-05, -2.33744690e-03, ...,\n",
       "         -1.77308882e-03, -2.15978012e-03,  2.06259149e-03],\n",
       "        [-4.95946617e-04,  2.86490849e-05, -2.33744690e-03, ...,\n",
       "         -1.77308882e-03, -2.15978012e-03,  2.06259149e-03],\n",
       "        [-4.95946617e-04,  2.86490849e-05, -2.33744690e-03, ...,\n",
       "         -1.77308882e-03, -2.15978012e-03,  2.06259149e-03]],\n",
       "\n",
       "       [[ 1.20692406e-04,  6.11458381e-05,  5.36778170e-05, ...,\n",
       "         -2.62293561e-05, -2.12236555e-05,  4.41464108e-05],\n",
       "        [ 2.42754060e-04, -6.46849958e-06, -5.72653989e-05, ...,\n",
       "          3.90439855e-05, -6.51099690e-05, -6.69616929e-05],\n",
       "        [ 3.01919034e-04,  6.97595315e-05, -1.17498625e-04, ...,\n",
       "          5.36925581e-05, -7.74148502e-05, -1.57934031e-04],\n",
       "        ...,\n",
       "        [-4.95947432e-04,  2.86487793e-05, -2.33744690e-03, ...,\n",
       "         -1.77308917e-03, -2.15978012e-03,  2.06259126e-03],\n",
       "        [-4.95947432e-04,  2.86490122e-05, -2.33744690e-03, ...,\n",
       "         -1.77308894e-03, -2.15978012e-03,  2.06259149e-03],\n",
       "        [-4.95947432e-04,  2.86489103e-05, -2.33744690e-03, ...,\n",
       "         -1.77308917e-03, -2.15978012e-03,  2.06259126e-03]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터셋에서 데이터 한 배치만 불러오는 방법(model에 데이터를 아주 조금 넣어보는 것)\n",
    "# model의 input shape이 결정되면서 model.build()가 자동으로 호출됨.\n",
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "\n",
    "# 한 배치만 불러온 데이터를 모델에 넣어보기\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ee53085f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  1920128   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  131584    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  131584    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  1935129   \n",
      "=================================================================\n",
      "Total params: 4,118,425\n",
      "Trainable params: 4,118,425\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모델의 구조를 확인\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "837f9a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "687/687 [==============================] - 555s 801ms/step - loss: 0.4775 - val_loss: 0.1599\n",
      "Epoch 2/10\n",
      "687/687 [==============================] - 553s 804ms/step - loss: 0.1541 - val_loss: 0.1493\n",
      "Epoch 3/10\n",
      "687/687 [==============================] - 553s 805ms/step - loss: 0.1470 - val_loss: 0.1436\n",
      "Epoch 4/10\n",
      "687/687 [==============================] - 555s 807ms/step - loss: 0.1420 - val_loss: 0.1395\n",
      "Epoch 5/10\n",
      "687/687 [==============================] - 556s 808ms/step - loss: 0.1383 - val_loss: 0.1360\n",
      "Epoch 6/10\n",
      "687/687 [==============================] - 556s 809ms/step - loss: 0.1352 - val_loss: 0.1330\n",
      "Epoch 7/10\n",
      "687/687 [==============================] - 556s 809ms/step - loss: 0.1324 - val_loss: 0.1303\n",
      "Epoch 8/10\n",
      "687/687 [==============================] - 557s 811ms/step - loss: 0.1298 - val_loss: 0.1277\n",
      "Epoch 9/10\n",
      "687/687 [==============================] - 559s 813ms/step - loss: 0.1274 - val_loss: 0.1253\n",
      "Epoch 10/10\n",
      "687/687 [==============================] - 561s 816ms/step - loss: 0.1251 - val_loss: 0.1230\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam() \n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy( # 훈련 데이터의 라벨이 정수의 형태로 제공될 때 사용하는 손실함수\n",
    "    from_logits=True, # 기본값은 False. 모델에 의해 생성된 출력 값이 정규화되지 않았음을 손실 함수에 알려준다.(softmax함수가 적용되지 않았다는걸 의미) \n",
    "    reduction='none'  # 기본값은 SUM. 각자 나오는 값의 반환 원할 때 None을 사용한다.\n",
    ")\n",
    "# 모델을 학습시키키 위한 학습과정을 설정하는 단계\n",
    "model.compile(loss=loss, optimizer=optimizer) # 손실함수와 훈련과정을 설정\n",
    "hist = model.fit(dataset, epochs=10, validation_data=(enc_val, dec_val)) # 만들어둔 데이터셋으로 모델을 학습.(10번 학습을 반복)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "54adabb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 학습결과 시각화\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig, loss_ax = plt.subplots()\n",
    "\n",
    "loss_ax.plot(hist.history['loss'], label = 'train loss')\n",
    "loss_ax.plot(hist.history['val_loss'], label = 'val loss')\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bc9a7673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델에게 시작 문장을 전달하면 모델이 시작 문장을 바탕으로 작문을 진행하는 함수만들기\n",
    "def generate_text(model, tokenizer, init_sentence=\"\", max_len=20):  \n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence]) # init_sentence 텍스트 안의 단어들을 숫자의 시퀀스의 형태로 변환\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64) # 텐서로 변환\n",
    "    end_token = tokenizer.word_index[\"\"]\n",
    "\n",
    "    # 단어 하나씩 예측해 문장 만들기(루프를 돌면서 init_sentence에 단어를 하나씩 생성)\n",
    "    while True:\n",
    "        predict = model(test_tensor) # 입력받은 문장의 텐서 입력\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] # 예측된 값 중 가장 높은 확률인 word index 뽑아내기\n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1) # 예측된 word index를 문장 뒤에 붙이기\n",
    "        if predict_word.numpy()[0] == end_token: break # 모델이 를 예측했거나\n",
    "        if test_tensor.shape[1] >= max_len: break # max_len에 도달했다면 문장 생성을 마침\n",
    "            \n",
    "    generated = \"\"\n",
    "    # tokenizer를 이용해 word index를 단어로 하나씩 변환\n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated #최종적으로 모델이 생성한 문장을 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "949c6846",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i love t t meenie meenie meenie meenie meenie meenie meenie meenie meenie meenie meenie meenie meenie meenie meenie meenie '"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"i love\", max_len=20)\n",
    "# generate_text 함수에 model이라 정의한 모델을 이용해서 i love 로 시작되는 문장을 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5d2bf6",
   "metadata": {},
   "source": [
    "## Reference\n",
    "[문장분할](https://rfriend.tistory.com/748)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
