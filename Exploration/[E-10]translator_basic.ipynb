{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8f2c910",
   "metadata": {},
   "source": [
    "# 10강. 번역기를 만들어보자"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df38717d",
   "metadata": {},
   "source": [
    "## 10-7. 번역기 만들기 (1) 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f5a652",
   "metadata": {},
   "source": [
    "![데이터받아오기](https://user-images.githubusercontent.com/116326867/206612317-e9fb579f-6192-4dc5-9744-58cf060a9b6f.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74ccaf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86efab28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 197463\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "      <th>cc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>93433</th>\n",
       "      <td>Do you plan to buy that car?</td>\n",
       "      <td>Prévoyez-vous de faire l'acquisition de cette ...</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15237</th>\n",
       "      <td>Keep us covered.</td>\n",
       "      <td>Couvre-nous.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153171</th>\n",
       "      <td>Will you drink another cup of coffee?</td>\n",
       "      <td>Voulez-vous une autre tasse de café ?</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186185</th>\n",
       "      <td>No matter how long it takes, I will finish the...</td>\n",
       "      <td>Peu importe le temps que ça prend, je finirai ...</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157851</th>\n",
       "      <td>Do you think you'd like to work for us?</td>\n",
       "      <td>Pensez-vous que vous aimeriez travailler avec ...</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #6...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      eng  \\\n",
       "93433                        Do you plan to buy that car?   \n",
       "15237                                    Keep us covered.   \n",
       "153171              Will you drink another cup of coffee?   \n",
       "186185  No matter how long it takes, I will finish the...   \n",
       "157851            Do you think you'd like to work for us?   \n",
       "\n",
       "                                                      fra  \\\n",
       "93433   Prévoyez-vous de faire l'acquisition de cette ...   \n",
       "15237                                        Couvre-nous.   \n",
       "153171              Voulez-vous une autre tasse de café ?   \n",
       "186185  Peu importe le temps que ça prend, je finirai ...   \n",
       "157851  Pensez-vous que vous aimeriez travailler avec ...   \n",
       "\n",
       "                                                       cc  \n",
       "93433   CC-BY 2.0 (France) Attribution: tatoeba.org #1...  \n",
       "15237   CC-BY 2.0 (France) Attribution: tatoeba.org #7...  \n",
       "153171  CC-BY 2.0 (France) Attribution: tatoeba.org #6...  \n",
       "186185  CC-BY 2.0 (France) Attribution: tatoeba.org #3...  \n",
       "157851  CC-BY 2.0 (France) Attribution: tatoeba.org #6...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "file_path = os.getenv('HOME')+'/aiffel/translator_seq2seq/data/fra.txt'\n",
    "lines = pd.read_csv(file_path, names=['eng', 'fra', 'cc'], sep='\\t')\n",
    "print('전체 샘플의 수 :',len(lines))\n",
    "lines.sample(5) #샘플 5개 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aed99edc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32397</th>\n",
       "      <td>This is way better.</td>\n",
       "      <td>C'est bien mieux.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37572</th>\n",
       "      <td>My friend helped me.</td>\n",
       "      <td>Mon amie m'a aidée.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26750</th>\n",
       "      <td>Tom likes country.</td>\n",
       "      <td>Tom aime la country.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26012</th>\n",
       "      <td>That's undeniable.</td>\n",
       "      <td>C'est indéniable.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38457</th>\n",
       "      <td>There's room inside.</td>\n",
       "      <td>Il y a de la place à l'intérieur.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        eng                                fra\n",
       "32397   This is way better.                  C'est bien mieux.\n",
       "37572  My friend helped me.                Mon amie m'a aidée.\n",
       "26750    Tom likes country.               Tom aime la country.\n",
       "26012    That's undeniable.                  C'est indéniable.\n",
       "38457  There's room inside.  Il y a de la place à l'intérieur."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = lines[['eng', 'fra']][:50000] # 5만개 샘플 사용\n",
    "lines.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d530ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 50000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28009</th>\n",
       "      <td>You're courageous.</td>\n",
       "      <td>\\t Tu es courageuse. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13328</th>\n",
       "      <td>Congratulations!</td>\n",
       "      <td>\\t Toutes nos félicitations ! \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34652</th>\n",
       "      <td>Do exactly as I say.</td>\n",
       "      <td>\\t Fais précisément ce que je dis. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25122</th>\n",
       "      <td>It's a good movie.</td>\n",
       "      <td>\\t C'est un bon film. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25801</th>\n",
       "      <td>Stand up straight.</td>\n",
       "      <td>\\t Tenez-vous droite ! \\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        eng                                    fra\n",
       "28009    You're courageous.                \\t Tu es courageuse. \\n\n",
       "13328      Congratulations!       \\t Toutes nos félicitations ! \\n\n",
       "34652  Do exactly as I say.  \\t Fais précisément ce que je dis. \\n\n",
       "25122    It's a good movie.               \\t C'est un bon film. \\n\n",
       "25801    Stand up straight.              \\t Tenez-vous droite ! \\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 시작 토큰<sos>과 종료 토큰<eos> 추가\n",
    "sos_token = '\\t'\n",
    "eos_token = '\\n'\n",
    "lines.fra = lines.fra.apply(lambda x : '\\t ' + x + ' \\n')\n",
    "# fra 각 열에 앞 뒤로 각각 \\t와 \\n을 넣기 \n",
    "\n",
    "print('전체 샘플의 수 :',len(lines))\n",
    "lines.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76c9fd6",
   "metadata": {},
   "source": [
    "- [apply(lambda x)](https://ordo.tistory.com/128)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c45ce1b",
   "metadata": {},
   "source": [
    "각 단어에 부여된 고유한 정수로 텍스트 시퀀스를 정수 시퀀스로 변환하는 정수 인코딩 과정을 거친 후 단어장 만들기(영어와 프랑스어는 사용하는 언어가 다르므로 단어장을 별도로 만든다)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ced6560c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[19, 3, 8], [19, 3, 8], [19, 3, 8]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 영어\n",
    "eng_tokenizer = Tokenizer(char_level=True)   # 문자 단위로 Tokenizer를 생성\n",
    "eng_tokenizer.fit_on_texts(lines.eng)               # 50000개의 행을 가진 eng의 각 행에 토큰화를 수행\n",
    "input_text = eng_tokenizer.texts_to_sequences(lines.eng)    # 단어를 숫자값 인덱스로 변환하여 저장\n",
    "input_text[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d774a6e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[10, 1, 19, 5, 1, 31, 1, 11],\n",
       " [10, 1, 15, 5, 12, 16, 29, 2, 14, 1, 11],\n",
       " [10, 1, 2, 7, 1, 12, 9, 8, 4, 2, 1, 31, 1, 11]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 프랑스어\n",
    "fra_tokenizer = Tokenizer(char_level=True)   # 문자 단위로 Tokenizer를 생성\n",
    "fra_tokenizer.fit_on_texts(lines.fra)                 # 50000개의 행을 가진 fra의 각 행에 토큰화를 수행\n",
    "target_text = fra_tokenizer.texts_to_sequences(lines.fra)     # 단어를 숫자값 인덱스로 변환하여 저장\n",
    "target_text[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b96d25bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 단어장의 크기 : 53\n",
      "프랑스어 단어장의 크기 : 73\n"
     ]
    }
   ],
   "source": [
    "# 단어장 만들기\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "fra_vocab_size = len(fra_tokenizer.word_index) + 1\n",
    "print('영어 단어장의 크기 :', eng_vocab_size)\n",
    "print('프랑스어 단어장의 크기 :', fra_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bae7cfcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 시퀀스의 최대 길이 22\n",
      "프랑스어 시퀀스의 최대 길이 76\n"
     ]
    }
   ],
   "source": [
    "# 영어 데이터와 프랑스어 데이터의 최대 길이(for 패딩)\n",
    "max_eng_seq_len = max([len(line) for line in input_text])\n",
    "max_fra_seq_len = max([len(line) for line in target_text])\n",
    "print('영어 시퀀스의 최대 길이', max_eng_seq_len)\n",
    "print('프랑스어 시퀀스의 최대 길이', max_fra_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5966aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 50000\n",
      "영어 단어장의 크기 : 53\n",
      "프랑스어 단어장의 크기 : 73\n",
      "영어 시퀀스의 최대 길이 22\n",
      "프랑스어 시퀀스의 최대 길이 76\n"
     ]
    }
   ],
   "source": [
    "# 전체적인 통계 정보\n",
    "print('전체 샘플의 수 :',len(lines))\n",
    "print('영어 단어장의 크기 :', eng_vocab_size)\n",
    "print('프랑스어 단어장의 크기 :', fra_vocab_size)\n",
    "print('영어 시퀀스의 최대 길이', max_eng_seq_len)\n",
    "print('프랑스어 시퀀스의 최대 길이', max_fra_seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafc728f",
   "metadata": {},
   "source": [
    "인코더의 입력으로 사용되는 영어 시퀀스와 달리, 프랑스어 시퀀스는 2가지 버전으로 나누어 준비해야 한다. 하나는 디코더의 출력(< sos > 필요 없음)과 비교해야 할 정답 데이터로 사용하고, 다른 하나는 교사 강요(Teacher forcing)를 위해 디코더의 입력(< eos > 필요 없음)으로 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ad14076",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = input_text\n",
    "# 종료 토큰 제거\n",
    "decoder_input = [[ char for char in line if char != fra_tokenizer.word_index[eos_token] ] for line in target_text] \n",
    "# 시작 토큰 제거\n",
    "decoder_target = [[ char for char in line if char != fra_tokenizer.word_index[sos_token] ] for line in target_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4a380d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10, 1, 19, 5, 1, 31, 1], [10, 1, 15, 5, 12, 16, 29, 2, 14, 1], [10, 1, 2, 7, 1, 12, 9, 8, 4, 2, 1, 31, 1]]\n",
      "[[1, 19, 5, 1, 31, 1, 11], [1, 15, 5, 12, 16, 29, 2, 14, 1, 11], [1, 2, 7, 1, 12, 9, 8, 4, 2, 1, 31, 1, 11]]\n"
     ]
    }
   ],
   "source": [
    "print(decoder_input[:3])\n",
    "print(decoder_target[:3])\n",
    "# 디코더 입력은 4(< eos > 토큰) 제거, 디코더 출력은 3(< sos > 토큰) 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fabd4004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 데이터의 크기(shape) : (50000, 22)\n",
      "프랑스어 입력데이터의 크기(shape) : (50000, 76)\n",
      "프랑스어 출력데이터의 크기(shape) : (50000, 76)\n"
     ]
    }
   ],
   "source": [
    "# 패딩하기\n",
    "encoder_input = pad_sequences(encoder_input, maxlen = max_eng_seq_len, padding='post')\n",
    "decoder_input = pad_sequences(decoder_input, maxlen = max_fra_seq_len, padding='post')\n",
    "decoder_target = pad_sequences(decoder_target, maxlen = max_fra_seq_len, padding='post')\n",
    "print('영어 데이터의 크기(shape) :',np.shape(encoder_input))\n",
    "print('프랑스어 입력데이터의 크기(shape) :',np.shape(decoder_input))\n",
    "print('프랑스어 출력데이터의 크기(shape) :',np.shape(decoder_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f1c99f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19  3  8  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "print(encoder_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0efab633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 데이터의 크기(shape) : (50000, 22, 53)\n",
      "프랑스어 입력데이터의 크기(shape) : (50000, 76, 73)\n",
      "프랑스어 출력데이터의 크기(shape) : (50000, 76, 73)\n"
     ]
    }
   ],
   "source": [
    "# 각 정수에 대해서 벡터화 방법으로 원-핫 인코딩\n",
    "encoder_input = to_categorical(encoder_input)\n",
    "decoder_input = to_categorical(decoder_input)\n",
    "decoder_target = to_categorical(decoder_target)\n",
    "print('영어 데이터의 크기(shape) :',np.shape(encoder_input))\n",
    "print('프랑스어 입력데이터의 크기(shape) :',np.shape(decoder_input))\n",
    "print('프랑스어 출력데이터의 크기(shape) :',np.shape(decoder_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b47c71",
   "metadata": {},
   "source": [
    "- 원-핫 인코딩 후 데이터의 크기는 (샘플 수 x 샘플의 길이 x 단어장의 크기)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99da0611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 학습데이터의 크기(shape) : (50000, 22, 53)\n",
      "프랑스어 학습 입력데이터의 크기(shape) : (50000, 76, 73)\n",
      "프랑스어 학습 출력데이터의 크기(shape) : (50000, 76, 73)\n"
     ]
    }
   ],
   "source": [
    "# 50000건 중 3000건 검증데이터로, 나머지를 학습데이터로\n",
    "n_of_val = 3000\n",
    "\n",
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]\n",
    "\n",
    "print('영어 학습데이터의 크기(shape) :',np.shape(encoder_input))\n",
    "print('프랑스어 학습 입력데이터의 크기(shape) :',np.shape(decoder_input))\n",
    "print('프랑스어 학습 출력데이터의 크기(shape) :',np.shape(decoder_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b12ce8f",
   "metadata": {},
   "source": [
    "## 10-8. 번역기 만들기 (2) 모델 훈련하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18d63f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더 설계\n",
    "# 입력 텐서 생성(입력 문장을 저장하게 될 변수 텐서)\n",
    "encoder_inputs = Input(shape=(None, eng_vocab_size))\n",
    "# hidden size가 256인 인코더의 LSTM 셀 생성, hidden state와 cell state 리턴받을 수 있도록 함\n",
    "encoder_lstm = LSTM(units = 256, return_state = True)\n",
    "# 디코더로 전달할 hidden state, cell state를 리턴. encoder_outputs은 여기서는 불필요.\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "# hidden state와 cell state를 다음 time step으로 전달하기 위해서 별도 저장.\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab2ab3a",
   "metadata": {},
   "source": [
    "- RNN의 경우 인코더의 마지막 hidden state를 디코더의 첫번째 hidden state로 사용한다. LSTM의 경우에는 cell state도 존재하기 때문에 인코더 LSTM 셀의 마지막 time step의 hidden state와 cell state를 디코더 LSTM의 첫번째 hidden state와 cell state로 전달해주어야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d8fc0a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None, 53)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None, 73)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 256), (None, 317440      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 256),  337920      input_2[0][0]                    \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 73)     18761       lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 674,121\n",
      "Trainable params: 674,121\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 디코더 설계\n",
    "# 입력 텐서 생성.\n",
    "decoder_inputs = Input(shape=(None, fra_vocab_size))\n",
    "# hidden size가 256인 인코더의 LSTM 셀 생성\n",
    "decoder_lstm = LSTM(units = 256, return_sequences = True, return_state=True)\n",
    "# decoder_outputs는 모든 time step의 hidden state\n",
    "decoder_outputs, _, _= decoder_lstm(decoder_inputs, initial_state = encoder_states)\n",
    "\n",
    "# 디코더 출력층\n",
    "decoder_softmax_layer = Dense(fra_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
    "\n",
    "# 인코더와 디코더 연결하기\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6925d94e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "368/368 [==============================] - 31s 20ms/step - loss: 1.0020 - val_loss: 0.8798\n",
      "Epoch 2/50\n",
      "368/368 [==============================] - 6s 18ms/step - loss: 0.6329 - val_loss: 0.7185\n",
      "Epoch 3/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.5371 - val_loss: 0.6447\n",
      "Epoch 4/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.4889 - val_loss: 0.6030\n",
      "Epoch 5/50\n",
      "368/368 [==============================] - 6s 18ms/step - loss: 0.4554 - val_loss: 0.5675\n",
      "Epoch 6/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.4294 - val_loss: 0.5451\n",
      "Epoch 7/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.4070 - val_loss: 0.5163\n",
      "Epoch 8/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.3881 - val_loss: 0.4953\n",
      "Epoch 9/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.3709 - val_loss: 0.4764\n",
      "Epoch 10/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.3557 - val_loss: 0.4615\n",
      "Epoch 11/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.3419 - val_loss: 0.4484\n",
      "Epoch 12/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.3295 - val_loss: 0.4336\n",
      "Epoch 13/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.3176 - val_loss: 0.4223\n",
      "Epoch 14/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.3070 - val_loss: 0.4126\n",
      "Epoch 15/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.2969 - val_loss: 0.4039\n",
      "Epoch 16/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.2871 - val_loss: 0.3961\n",
      "Epoch 17/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.2781 - val_loss: 0.3872\n",
      "Epoch 18/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.2702 - val_loss: 0.3830\n",
      "Epoch 19/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.2625 - val_loss: 0.3803\n",
      "Epoch 20/50\n",
      "368/368 [==============================] - 7s 19ms/step - loss: 0.2553 - val_loss: 0.3708\n",
      "Epoch 21/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.2485 - val_loss: 0.3674\n",
      "Epoch 22/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.2423 - val_loss: 0.3666\n",
      "Epoch 23/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.2363 - val_loss: 0.3605\n",
      "Epoch 24/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.2309 - val_loss: 0.3579\n",
      "Epoch 25/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.2254 - val_loss: 0.3566\n",
      "Epoch 26/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.2205 - val_loss: 0.3534\n",
      "Epoch 27/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.2157 - val_loss: 0.3551\n",
      "Epoch 28/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.2114 - val_loss: 0.3539\n",
      "Epoch 29/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.2071 - val_loss: 0.3537\n",
      "Epoch 30/50\n",
      "368/368 [==============================] - 7s 19ms/step - loss: 0.2029 - val_loss: 0.3526\n",
      "Epoch 31/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.1990 - val_loss: 0.3516\n",
      "Epoch 32/50\n",
      "368/368 [==============================] - 7s 19ms/step - loss: 0.1952 - val_loss: 0.3539\n",
      "Epoch 33/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.1915 - val_loss: 0.3517\n",
      "Epoch 34/50\n",
      "368/368 [==============================] - 7s 19ms/step - loss: 0.1884 - val_loss: 0.3525\n",
      "Epoch 35/50\n",
      "368/368 [==============================] - 7s 19ms/step - loss: 0.1849 - val_loss: 0.3549\n",
      "Epoch 36/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.1816 - val_loss: 0.3562\n",
      "Epoch 37/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.1788 - val_loss: 0.3549\n",
      "Epoch 38/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.1759 - val_loss: 0.3581\n",
      "Epoch 39/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.1728 - val_loss: 0.3605\n",
      "Epoch 40/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.1702 - val_loss: 0.3611\n",
      "Epoch 41/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.1675 - val_loss: 0.3629\n",
      "Epoch 42/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.1649 - val_loss: 0.3640\n",
      "Epoch 43/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.1627 - val_loss: 0.3657\n",
      "Epoch 44/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.1601 - val_loss: 0.3680\n",
      "Epoch 45/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.1576 - val_loss: 0.3734\n",
      "Epoch 46/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.1557 - val_loss: 0.3720\n",
      "Epoch 47/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.1534 - val_loss: 0.3762\n",
      "Epoch 48/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.1511 - val_loss: 0.3784\n",
      "Epoch 49/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.1493 - val_loss: 0.3821\n",
      "Epoch 50/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.1471 - val_loss: 0.3829\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f94999f1c40>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습하기\n",
    "model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train, \\\n",
    "         validation_data = ([encoder_input_test, decoder_input_test], decoder_target_test),\n",
    "         batch_size=128, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af11303e",
   "metadata": {},
   "source": [
    "## 10-9. 번역기 만들기 (3) 모델 테스트하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb8cbfa",
   "metadata": {},
   "source": [
    "테스트 단계의 디코더 모델은 다시 설계를 해주어야 한다. 훈련시에는 학습해야 할 타겟 문장을 디코더 모델의 입력, 출력 시퀀스로 넣어 주고, 디코더 모델이 타겟 문장을 한꺼번에 출력하게 할 수 있는 반면, 테스트 단계에서는 그렇게 할 수가 없기 때문이다.  \n",
    "하나의 문장을 만들어 내기 위해 루프를 돌며 단어를 하나씩 차례로 예측하면서, 예측된 단어가 다음 입력으로 재사용되는 과정을 반복해주어야 한다.  \n",
    "1. 인코더에 입력 문장을 넣어 마지막 time step의 hidden, cell state 얻기\n",
    "2. < sos > 토큰인 \\t를 디코더에 입력\n",
    "3. 이전 time step의 출력층의 예측 결과를 현재 time step의 입력으로 하기\n",
    "4. 3번을 반복하다가 < eos > 토큰인 \\n이 예측되면 중단"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cc3b6308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, None, 53)]        0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  [(None, 256), (None, 256) 317440    \n",
      "=================================================================\n",
      "Total params: 317,440\n",
      "Trainable params: 317,440\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 인코더 정의\n",
    "encoder_model = Model(inputs = encoder_inputs, outputs = encoder_states)\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf33d60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더 설계\n",
    "# 이전 time step의 hidden state를 저장하는 텐서\n",
    "decoder_state_input_h = Input(shape=(256,))\n",
    "# 이전 time step의 cell state를 저장하는 텐서\n",
    "decoder_state_input_c = Input(shape=(256,))\n",
    "# 이전 time step의 hidden state와 cell state를 하나의 변수에 저장\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "# decoder_states_inputs를 현재 time step의 초기 상태로 사용\n",
    "# 구체적인 동작 자체는 def decode_sequence()에 구현\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state = decoder_states_inputs)\n",
    "# 현재 time step의 hidden state와 cell state를 하나의 변수에 저장\n",
    "decoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "26c88fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, None, 73)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 256),  337920      input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 73)     18761       lstm_1[1][0]                     \n",
      "==================================================================================================\n",
      "Total params: 356,681\n",
      "Trainable params: 356,681\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 디코더 출력층 재설계\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
    "decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs, outputs=[decoder_outputs] + decoder_states)\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cbad2249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어에서 정수로, 정수에서 단어로 바꾸는 사전 만들기(테스트 결과 해석을 위해서)\n",
    "eng2idx = eng_tokenizer.word_index\n",
    "fra2idx = fra_tokenizer.word_index\n",
    "idx2eng = eng_tokenizer.index_word\n",
    "idx2fra = fra_tokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d7c22015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decode_sequence() 함수만들기\n",
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # < SOS >에 해당하는 원-핫 벡터 생성\n",
    "    target_seq = np.zeros((1, 1, fra_vocab_size))\n",
    "    target_seq[0, 0, fra2idx['\\t']] = 1.\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "\n",
    "    # stop_condition이 True가 될 때까지 루프 반복\n",
    "    while not stop_condition:\n",
    "        # 이전 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # 예측 결과를 문자로 변환\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = idx2fra[sampled_token_index]\n",
    "\n",
    "        # 현재 시점의 예측 문자를 예측 문장에 추가\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # < eos >에 도달하거나 최대 길이를 넘으면 중단.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_fra_seq_len):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
    "        target_seq = np.zeros((1, 1, fra_vocab_size))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56971212",
   "metadata": {},
   "source": [
    "- [np.argmax](https://jimmy-ai.tistory.com/72) : 리스트 등의 원소 중 가장 큰 값의 인덱스를 반환한다. 가장 큰 원소가 여러개 있는 경우 가장 앞의 인덱스를 반환한다.  \n",
    "- [np.zeros](https://cosmosproject.tistory.com/408)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4644e0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "입력 문장: Go.\n",
      "정답 문장:  Bouge ! \n",
      "번역기가 번역한 문장:  marche. \n",
      "-----------------------------------\n",
      "입력 문장: Begin.\n",
      "정답 문장:  Commence. \n",
      "번역기가 번역한 문장:  commencez. \n",
      "-----------------------------------\n",
      "입력 문장: No way!\n",
      "정답 문장:  C'est exclu ! \n",
      "번역기가 번역한 문장:  c'est impressible ! \n",
      "-----------------------------------\n",
      "입력 문장: Have fun.\n",
      "정답 문장:  Amuse-toi bien ! \n",
      "번역기가 번역한 문장:  amusez-vous bien ! \n",
      "-----------------------------------\n",
      "입력 문장: Get off my car.\n",
      "정답 문장:  Descends de ma voiture. \n",
      "번역기가 번역한 문장:  lâchez-moi ! \n"
     ]
    }
   ],
   "source": [
    "# 출력 결과 테스트\n",
    "for seq_index in [3,46,132,576,10001]: # 입력 문장의 인덱스(변경 가능)\n",
    "    input_seq = encoder_input[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print(35 * \"-\")\n",
    "    print('입력 문장:', lines.eng[seq_index])\n",
    "    print('정답 문장:', lines.fra[seq_index][1:len(lines.fra[seq_index])-1]) # '\\t'와 '\\n'을 빼고 출력\n",
    "    print('번역기가 번역한 문장:', decoded_sentence[:len(decoded_sentence)-1]) # '\\n'을 빼고 출력"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
